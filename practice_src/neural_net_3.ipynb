{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN\n",
    "- Input to NN --> feature vector\n",
    "\n",
    "- 1D vector - Classic input to a neural network, similar to rows in a spreadsheet. Common in predictive modeling.\n",
    "- 2D Matrix - Grayscale image input to a CNN.\n",
    "- 3D Matrix - Color image input to a CNN.\n",
    "- nD Matrix - Higher-order input to a CNN.\n",
    "\n",
    "- Regression or two class classification --> networks always have a single output\n",
    "- Classification --> networks have an output neuron for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Neurons\n",
    "- Typically NN takes in floating-point vectors\n",
    "- Vector is 1D array eg: [0.75, 0.55, 0.2]\n",
    "\n",
    "### Hidden Neurons\n",
    "- Previously mentioned to have only one hidden layer --> Reason: training will take more time\n",
    "- Now we are computationaly well to do hence can have multiple layer of hidden neurons\n",
    "- Training refers to *__process that determines good weight values__*\n",
    "\n",
    "### Bias Neurons\n",
    "- Generally marked as 1 which is called the bias activation\n",
    "- Not connected with previous layers\n",
    "- Each layer is fixed with a bias neuron to provide a constant value\n",
    "- Allow the program to shift the output of an activation function\n",
    "\n",
    "### Neurons called as nodes, units or summations\n",
    "- __Input Neurons__: Map each input neuron to one element of feature vector\n",
    "- __Hidden Neurons__: Allows NN to be abstract and process input to the output\n",
    "- __Output Neurons__: Each output neuron calculate one part of the output\n",
    "- __Bias Neurons__: Work similar to y-intercept of linear equation (y = mx + b) i.e. the *b* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Output Neurons\n",
    "- Input neurons will have each neuron take an input vector and output the output vector in similar shape of the input vector\n",
    "- Shape of input vector or array must be similar to the number of input neurons\n",
    "    - Eg: Three input neurons must have:\n",
    "        [0.5, 0.75, 0.3]\n",
    "\n",
    "### Hidden Neurons\n",
    "- Takes input from input neurons or other hidden neurons\n",
    "- Helps to understand input and form the output\n",
    "- Most of time, bunch multiple hidden layers\n",
    "- Training means finding the best weights values\n",
    "\n",
    "### Bias Neurons\n",
    "- Fixed output of value like 1\n",
    "- Each layer apart from output layer will have bias neurons\n",
    "- Not connected with previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Bias neurons needed?\n",
    "- Activation function specifies the output of a neuron\n",
    "- Derivative of a function is taken to measure its *senstivity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "- ReLU: Used for output of hidden layers\n",
    "- Softmax: Used for output of classification\n",
    "- Linear: Used for output of regression (or 2-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = x \n",
    "$$ \n",
    "- Linear activation function spits out a constant value when tried to apply backpropogation as derivative is a constant\n",
    "- Last layer will be a linear function of the first layer: hence linear function turns the NN into just one layer\n",
    "\n",
    "<img src='../misc/Linear.png' width='800'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Units (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = max(0,x)\n",
    "$$\n",
    "- Good for hidden layers\n",
    "\n",
    "<img src='../misc/ReLU.png' width='800'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generally found in classification based problems with usually present in the output layer of NN\n",
    "- Each neuron gives its output based on *probability* of each class\n",
    "- The probability will sum to 100%\n",
    "- 1 output neuron for each class\n",
    "\n",
    "$$\n",
    "    f*i(x) = \\frac{exp(x_i)} {\\sum*j exp(x_j)}\n",
    "$$\n",
    "\n",
    "- The output of each neuron is not the probability but the output vector and if you apply the above formula then it will give the probability of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step activation is 1 if x >= 0.5 and 0 otherwise\n",
    "- Mainly used in binary classification\n",
    "\n",
    "<img src='../misc/Step.png' width='800'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "- Also called Logistic activation function\n",
    "- Use to enusre that values stay within a relatively small range like 0 to 1 (most of the time)\n",
    "\n",
    "<img src='../misc/sigmoid.png' width='800'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Outputs the value in range -1 to 1\n",
    "\n",
    "$$\n",
    "    f(x) = tanh(x)\n",
    "$$\n",
    "- HTAN/tanh has many advantages over Sigmoid:\n",
    "    - tanh converges faster than sigmoid\n",
    "    \n",
    "Reference: [Sigmoid vs Tanh](https://www.baeldung.com/cs/sigmoid-vs-tanh-functions)\n",
    "\n",
    "<img src='../misc/tanh.png' width='800'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivating of Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, apply the reciprocal rule.\n",
    "\n",
    "$$\n",
    "s’(x) = \\frac{d}{dx} \\left( \\frac{1}{e^{-x}+1} \\right) = \\frac{-\\frac{d}{dx}(e^{-x}+1)}{(e^{-x}+1)^2}\n",
    "$$\n",
    "\n",
    "Second, this is just a linear equation, so we can take the derivative of each part. Constant 1 goes away.\n",
    "\n",
    "$$\n",
    "= \\frac{-\\frac{d}{dx}(e^{-x})}{(e^{-x}+1)^2}\n",
    "$$\n",
    "\n",
    "Next, exponential function rule with chain rule.\n",
    "\n",
    "$$\n",
    "= \\frac{-( e^{-x} \\cdot \\frac{d}{dx}({-x}))}{(e^{-x}+1)^2}\n",
    "$$\n",
    "\n",
    "Next, this is just a linear equation, so we can take the derivative of each part.\n",
    "\n",
    "$$\n",
    "= \\frac{-(-1)\\cdot{e}^{-x}}{(e^{-x}+1)^2} =\n",
    "$$\n",
    "\n",
    "Eliminate the double negative.\n",
    "\n",
    "$$\n",
    "= \\frac{e^{-x}}{(e^{-x}+1)^2}\n",
    "$$\n",
    "\n",
    "We could stop with the above derivative. However, most texts do not display the above derivative as the final form of the sigmoid functions derivative. For “computational efficiency” we algebraically transform this to use the original sigmoid function twice.\n",
    "\n",
    "$$\n",
    "= \\left( \\frac{1}{e^{-x}+1} \\right) \\left( \\frac{-e^{-x}}{e^{-x}+1} \\right)\n",
    "$$\n",
    "\n",
    "This now reduces to the commonly used form of the sigmoid’s derivative.\n",
    "\n",
    "$$\n",
    "s’(x)= s(x)(1-s(x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "- Commonly used as output are changed to probability values rather than *logits* (vector of raw scores)\n",
    "- Calculated by exponentiating each score and then normalizing the results to sum to 1\n",
    "$$\n",
    "    P(y_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "*$P(y_i)$* is the probability of class \n",
    "\n",
    "*$z_i$* is the raw score (logit) for class \n",
    "\n",
    "*$K$* is the total number of classes.\n",
    "\n",
    "### LogSoftmax\n",
    "- Better as compared to softmax\n",
    "- No need to compare probabilities \n",
    "- Prevent overflow or underflow errors\n",
    "- Log softmax is also useful when applying the negative log-likelihood loss function, as it simplifies the mathematical calculations by eliminating the need to compute the actual probabilities.\n",
    "$$\n",
    "    \\log P(y_i) = {z_i} - log {\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss\n",
    "#### For Binary Classification:\n",
    "\n",
    "$$\n",
    "    CE(y,\\hat{y}) = -(y.\\log(y) + (1-y).\\log(1-\\hat{y}))\n",
    "$$\n",
    "where\n",
    "    $y$ is true label (0 or 1)\n",
    "    $\\hat{y} is predicted probability of belonging to class 1\n",
    "\n",
    "#### For Multi-class classification\n",
    "- Often called *categorical cross entropy loss* or *softmax loss*\n",
    "$$\n",
    "    CE(y,\\hat{y}) = -\\sum_{i=1}^N y_i. \\log{\\hat{y_i}}\n",
    "$$\n",
    "where\n",
    "    $N$ is number of classes\n",
    "    $y_i$ is true probability distribution over the classes\n",
    "    $\\hat{y_i}$ is predicted probability distribution over the classes produced by softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch\n",
    "\n",
    "- When initiating an uninitialized tensor use *__torch.Tensor__*\n",
    "- When inititating or creating a tensor from existing data and ensure the data is successfully created then use *__torch.tensor__*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch training loop explaination\n",
    "```\n",
    "# training loop\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "Explaination:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
