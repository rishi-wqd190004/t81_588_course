{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN\n",
    "- Input to NN --> feature vector\n",
    "\n",
    "- 1D vector - Classic input to a neural network, similar to rows in a spreadsheet. Common in predictive modeling.\n",
    "- 2D Matrix - Grayscale image input to a CNN.\n",
    "- 3D Matrix - Color image input to a CNN.\n",
    "- nD Matrix - Higher-order input to a CNN.\n",
    "\n",
    "- Regression or two class classification --> networks always have a single output\n",
    "- Classification --> networks have an output neuron for each category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Neurons\n",
    "- Typically NN takes in floating-point vectors\n",
    "- Vector is 1D array eg: [0.75, 0.55, 0.2]\n",
    "\n",
    "### Hidden Neurons\n",
    "- Previously mentioned to have only one hidden layer --> Reason: training will take more time\n",
    "- Now we are computationaly well to do hence can have multiple layer of hidden neurons\n",
    "- Training refers to *__process that determines good weight values__*\n",
    "\n",
    "### Bias Neurons\n",
    "- Generally marked as 1 which is called the bias activation\n",
    "- Not connected with previous layers\n",
    "- Each layer is fixed with a bias neuron to provide a constant value\n",
    "- Allow the program to shift the output of an activation function\n",
    "\n",
    "### Neurons called as nodes, units or summations\n",
    "- __Input Neurons__: Map each input neuron to one element of feature vector\n",
    "- __Hidden Neurons__: Allows NN to be abstract and process input to the output\n",
    "- __Output Neurons__: Each output neuron calculate one part of the output\n",
    "- __Bias Neurons__: Work similar to y-intercept of linear equation (y = mx + b) i.e. the *b* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and Output Neurons\n",
    "- Input neurons will have each neuron take an input vector and output the output vector in similar shape of the input vector\n",
    "- Shape of input vector or array must be similar to the number of input neurons\n",
    "    - Eg: Three input neurons must have:\n",
    "        [0.5, 0.75, 0.3]\n",
    "\n",
    "### Hidden Neurons\n",
    "- Takes input from input neurons or other hidden neurons\n",
    "- Helps to understand input and form the output\n",
    "- Most of time, bunch multiple hidden layers\n",
    "- Training means finding the best weights values\n",
    "\n",
    "### Bias Neurons\n",
    "- Fixed output of value like 1\n",
    "- Each layer apart from output layer will have bias neurons\n",
    "- Not connected with previous layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Bias neurons needed?\n",
    "- Activation function specifies the output of a neuron\n",
    "- Derivative of a function is taken to measure its *senstivity*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "- ReLU: Used for output of hidden layers\n",
    "- Softmax: Used for output of classification\n",
    "- Linear: Used for output of regression (or 2-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = x \n",
    "$$ \n",
    "- Linear activation function spits out a constant value when tried to apply backpropogation as derivative is a constant\n",
    "- Last layer will be a linear function of the first layer: hence linear function turns the NN into just one layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Units (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f(x) = max(0,x)\n",
    "$$\n",
    "- Good for hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generally found in classification based problems with usually present in the output layer of NN\n",
    "- Each neuron gives its output based on *probability* of each class\n",
    "- The probability will sum to 100%\n",
    "- 1 output neuron for each class\n",
    "\n",
    "$$\n",
    "    f*i(x) = \\frac{exp(x_i)} {\\sum*j exp(x_j)}\n",
    "$$\n",
    "\n",
    "- The output of each neuron is not the probability but the output vector and if you apply the above formula then it will give the probability of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step activation is 1 if x >= 0.5 and 0 otherwise\n",
    "- Mainly used in binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
